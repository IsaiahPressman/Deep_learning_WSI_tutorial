{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### TODO: Outline image here###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. [Libraries & Environment](#Libraries-&-Environment)\n",
    "1. [Data Preprocessing](#Data-Preprocessing)\n",
    "    1. Tiling\n",
    "    1. Filtering out background tiles\n",
    "    1. Macenko normalization\n",
    "    1. Tumor detection\n",
    "1. [Training Deep Learning Models](#Training-Deep-Learning-Models)\n",
    "    1. Data splitting\n",
    "    1. Model and data loading\n",
    "    1. Main training loop\n",
    "    1. Misc.\n",
    "1. [Evaluating Performance](#Evaluating-Performance)\n",
    "    1. Patient-level vs. tile-level evaluation\n",
    "    1. AUROC vs. accuracy\n",
    "    1. On improving performance\n",
    "1. [Visualizing Results](#Visualizing-Results)\n",
    "    1. TODO: outline\n",
    "1. TODO: Bonus section on XML annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries & Environment\n",
    "\n",
    "The base environment that I use can be installed using the create_conda_env.sh bash script.\n",
    "\n",
    "NB: As of June 2021, when installing OpenSlide on Linux, it will not work correctly with some image types due to a broken dependency. (I've noticed this problem for .mrxs images in particular) In order to repair this issue, you can install version 0.40.0 of the pixman library. (Installed automatically in the create_conda_env.sh script) If you notice the slide images look like like the image below, or throw an error when you view them, try this solution.\n",
    "\n",
    "TODO: insert image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from openslide import OpenSlide, OpenSlideError\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shutil\n",
    "import time\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Custom imports\n",
    "from library.MacenkoNormalizer import MacenkoNormalizer\n",
    "from library.model_utils import load_saved_model_for_inference, load_model_arch\n",
    "\n",
    "# DEVICE determines which GPU (or CPU) the deep learning code will be run on\n",
    "# DEVICE = torch.device('cpu')\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In order to prepare the WSI images for deep learning training and inference, a number of preprocessing steps must be applied:\n",
    "\n",
    "1. Images are broken into many small tiles (usually 256x256 microns)\n",
    "1. Tiles are filtered to exclude non-tissue background regions\n",
    "1. Tiles are Macenko-normalized\n",
    "1. Tiles are filtered to exclude non-tumorous tissue regions\n",
    "\n",
    "These steps are laid out in example code below. However, when applying this pipeline at scale, the implementation should include multiprocessing and/or CuPy (for Macenko normalization) as these additions provide enormous speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring WSIs/MSS/TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B.svs, as it has already been processed.\n",
      "Ignoring WSIs/MSS/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs, as it has already been processed.\n",
      "Ignoring WSIs/MSI-H/TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7.svs, as it has already been processed.\n",
      "Ignoring WSIs/MSI-H/TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014.svs, as it has already been processed.\n",
      "Masking and normalizing 0 tiles from 0 whole slide images.\n"
     ]
    }
   ],
   "source": [
    "# MICRONS_PER_TILE defines the tile edge length used when breaking WSIs into smaller images\n",
    "MICRONS_PER_TILE = 256.\n",
    "\n",
    "# Initialize the Macenko Normalizer\n",
    "reference_img = np.array(Image.open('library/macenko_reference_img.png').convert('RGB'))\n",
    "normalizer = MacenkoNormalizer()\n",
    "normalizer.fit(reference_img)\n",
    "\n",
    "# Find all WSIs and check for errors opening the file or finding the microns-per-pixel values \n",
    "base_path = Path('WSIs')\n",
    "base_save_path = Path('tiled_WSIs')\n",
    "wsi_paths = base_path.rglob('*.svs')\n",
    "save_paths = []\n",
    "wsi_paths_to_normalize = []\n",
    "total_num_tiles = 0\n",
    "for wsi_path in wsi_paths:\n",
    "    try:\n",
    "        with OpenSlide(str(wsi_path)) as wsi:\n",
    "            sub_path = Path(str(wsi_path)[len(str(base_path)) + 1:-len(wsi_path.suffix)])\n",
    "            save_path = base_save_path / sub_path\n",
    "\n",
    "            if (save_path / 'Finished.txt').exists():\n",
    "                print('Ignoring {}, as it has already been processed.'.format(wsi_path))\n",
    "            else:\n",
    "                pixels_per_tile_x = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-x']))\n",
    "                pixels_per_tile_y = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-y']))\n",
    "                wsi_paths_to_normalize.append(wsi_path)\n",
    "                save_paths.append(save_path)\n",
    "                save_path.mkdir(parents=True, exist_ok=True)\n",
    "                total_num_tiles += (\n",
    "                        len(range(pixels_per_tile_x, wsi.dimensions[0] - pixels_per_tile_x, pixels_per_tile_x)) *\n",
    "                        len(range(pixels_per_tile_y, wsi.dimensions[1] - pixels_per_tile_y, pixels_per_tile_y)))\n",
    "    except OpenSlideError:\n",
    "        print('Ignoring {}, as it cannot be opened by OpenSlide.'.format(wsi_path))\n",
    "    except KeyError:\n",
    "        print('Ignoring {}, as it does not have a defined microns-per-pixel value'.format(wsi_path))\n",
    "\n",
    "print(f'Masking and normalizing {total_num_tiles} tiles from {len(wsi_paths_to_normalize)} whole slide images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, given a whole slide image path and target save path, masks and normalizes all tissue tiles and then saves them into pngs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_normalize_wsi(wsi_path, save_path, pbar):\n",
    "    num_tiles_kept = 0\n",
    "    try:\n",
    "        with OpenSlide(str(wsi_path)) as wsi:\n",
    "            pptx = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-x']))\n",
    "            ppty = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-y']))\n",
    "            # Leave out border of image\n",
    "            for x in range(pptx, wsi.dimensions[0] - pptx, pptx):\n",
    "                for y in range(ppty, wsi.dimensions[1] - ppty, ppty):\n",
    "                    tile = wsi.read_region((x, y), level=0, size=(pptx, ppty)).convert('RGB')\n",
    "                    # Mask away all-white and all-black background regions\n",
    "                    mask = tile.convert(mode='L').point(lut=lambda p: 220 > p > 10, mode='1')\n",
    "                    mask = ndimage.binary_fill_holes(mask)\n",
    "                    if np.sum(mask).astype(float) / mask.size > 0.5:\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter('ignore')\n",
    "                            try:\n",
    "                                # Normalize the tile\n",
    "                                tile = normalizer.transform(np.array(tile))\n",
    "                                tile = Image.fromarray(tile)\n",
    "                                # Resize the image to 224x224\n",
    "                                tile = tile.resize((224, 224), Image.LANCZOS)\n",
    "                                num_tiles_kept += 1\n",
    "                                filename = f'{wsi_path.stem}__x{x}_y{y}_dx{pptx}_dy{ppty}.png'\n",
    "                                tile.save(save_path / filename, format='PNG')\n",
    "                            except np.linalg.LinAlgError:\n",
    "                                pass\n",
    "                    pbar.update()\n",
    "    except OpenSlideError as ex:\n",
    "        print('\\nUnable to process {}:'.format(wsi_path))\n",
    "        print(''.join(traceback.format_exception(etype=type(ex), value=ex, tb=ex.__traceback__)))\n",
    "        shutil.rmtree(save_path)\n",
    "        return 0\n",
    "\n",
    "    with open(save_path / 'Finished.txt', 'w+') as file:\n",
    "        file.write('Kept and processed {} tiles.'.format(num_tiles_kept))\n",
    "    return num_tiles_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2051 tiles from patient TCGA-4N-A93T-01Z-00-DX1 saved to tiled_WSIs/MSS/TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B\n",
      "2960 tiles from patient TCGA-3L-AA1B-01Z-00-DX1 saved to tiled_WSIs/MSS/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F\n",
      "4172 tiles from patient TCGA-5M-AAT6-01Z-00-DX1 saved to tiled_WSIs/MSI-H/TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014\n",
      "6951 tiles from patient TCGA-5M-AATE-01Z-00-DX1 saved to tiled_WSIs/MSI-H/TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7\n",
      "16134 tiles were saved and normalized\n"
     ]
    }
   ],
   "source": [
    "assert len(wsi_paths_to_normalize) == len(save_paths)\n",
    "with tqdm.tqdm(total=total_num_tiles) as pbar:\n",
    "    for wsi_path, save_path in zip(wsi_paths_to_normalize, save_paths):\n",
    "        mask_and_normalize_wsi(wsi_path, save_path, pbar)\n",
    "# Wait a moment for pbar to close\n",
    "time.sleep(0.25)\n",
    "\n",
    "all_save_paths = [p for p in base_save_path.glob('*/*') if p.is_dir()]\n",
    "total_tiles_kept = 0\n",
    "for save_path in all_save_paths:\n",
    "    with open(save_path / 'Finished.txt', 'r') as f:\n",
    "        info = f.readline()\n",
    "    num_tiles_kept = int(re.search('processed ([0-9]+?) tiles', info).group(1))\n",
    "    total_tiles_kept += num_tiles_kept\n",
    "    print(f'{num_tiles_kept} tiles from patient {save_path.stem} saved to {save_path}')\n",
    "print(f'{total_tiles_kept} tiles were saved and normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that WSIs have been broken into normalized tiles, we load these images for tumor detection.\n",
    "\n",
    "NB: Make sure to use `with torch.no_grad():` at inference time or there may be memory overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for tumor detection...\n",
      "Getting tumor predictions for 16134 tiles in 127 batches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:08<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9806/16134 tiles contain tumorous tissue\n"
     ]
    }
   ],
   "source": [
    "print('Loading images for tumor detection...')\n",
    "img_dataset = datasets.ImageFolder(\n",
    "    base_save_path,\n",
    "    transforms.Compose([\n",
    "        # Images must be of size 224x224 to be passed to most deep learning vision models\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "img_dataloader = data.DataLoader(\n",
    "    img_dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "tumor_detection_model = load_saved_model_for_inference(\n",
    "    'saved_models/resnet18_tumor_detection_exp9.pt',\n",
    "    num_classes=2,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f'Getting tumor predictions for {len(img_dataset)} tiles in {len(img_dataloader)} batches.')\n",
    "time.sleep(0.25)\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in tqdm.tqdm(img_dataloader):\n",
    "        inputs = inputs.to(DEVICE, non_blocking=True)\n",
    "        outputs = tumor_detection_model(inputs).cpu()\n",
    "        all_preds.append(outputs)\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "time.sleep(0.25)\n",
    "\n",
    "tumorous_tiles = all_preds.argmax(dim=1).flatten()\n",
    "print(f'{tumorous_tiles.sum()}/{len(img_dataset)} tiles contain tumorous tissue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tile_info_df to \"tiled_WSIs/tile_info.csv\"\n",
      "Saved patient_info_df to \"tiled_WSIs/patient_info.csv\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_0bf45_\" ><caption>Patient info dataframe</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >MSI_status</th>    </tr>    <tr>        <th class=\"index_name level0\" >patient_id</th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0bf45_level0_row0\" class=\"row_heading level0 row0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014</th>\n",
       "                        <td id=\"T_0bf45_row0_col0\" class=\"data row0 col0\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0bf45_level0_row1\" class=\"row_heading level0 row1\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7</th>\n",
       "                        <td id=\"T_0bf45_row1_col0\" class=\"data row1 col0\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0bf45_level0_row2\" class=\"row_heading level0 row2\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F</th>\n",
       "                        <td id=\"T_0bf45_row2_col0\" class=\"data row2 col0\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0bf45_level0_row3\" class=\"row_heading level0 row3\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B</th>\n",
       "                        <td id=\"T_0bf45_row3_col0\" class=\"data row3 col0\" >MSS</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f64eb891990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_c0b1f_\" ><caption>Tile info dataframe example rows</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >patient_id</th>        <th class=\"col_heading level0 col1\" >tumor_pred_val</th>        <th class=\"col_heading level0 col2\" >tumor_pred_class</th>        <th class=\"col_heading level0 col3\" >MSI_status</th>    </tr>    <tr>        <th class=\"index_name level0\" >tile_id</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row0\" class=\"row_heading level0 row0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014__x100287_y12156_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row0_col0\" class=\"data row0 col0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014</td>\n",
       "                        <td id=\"T_c0b1f_row0_col1\" class=\"data row0 col1\" >0.010808</td>\n",
       "                        <td id=\"T_c0b1f_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "                        <td id=\"T_c0b1f_row0_col3\" class=\"data row0 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row1\" class=\"row_heading level0 row1\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014__x100287_y13169_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row1_col0\" class=\"data row1 col0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014</td>\n",
       "                        <td id=\"T_c0b1f_row1_col1\" class=\"data row1 col1\" >0.045893</td>\n",
       "                        <td id=\"T_c0b1f_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "                        <td id=\"T_c0b1f_row1_col3\" class=\"data row1 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row2\" class=\"row_heading level0 row2\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7__x100287_y10130_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row2_col0\" class=\"data row2 col0\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7</td>\n",
       "                        <td id=\"T_c0b1f_row2_col1\" class=\"data row2 col1\" >0.999991</td>\n",
       "                        <td id=\"T_c0b1f_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "                        <td id=\"T_c0b1f_row2_col3\" class=\"data row2 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row3\" class=\"row_heading level0 row3\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7__x100287_y11143_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row3_col0\" class=\"data row3 col0\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7</td>\n",
       "                        <td id=\"T_c0b1f_row3_col1\" class=\"data row3 col1\" >0.999997</td>\n",
       "                        <td id=\"T_c0b1f_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "                        <td id=\"T_c0b1f_row3_col3\" class=\"data row3 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row4\" class=\"row_heading level0 row4\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F__x10130_y33429_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row4_col0\" class=\"data row4 col0\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F</td>\n",
       "                        <td id=\"T_c0b1f_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "                        <td id=\"T_c0b1f_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "                        <td id=\"T_c0b1f_row4_col3\" class=\"data row4 col3\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row5\" class=\"row_heading level0 row5\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F__x10130_y34442_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row5_col0\" class=\"data row5 col0\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F</td>\n",
       "                        <td id=\"T_c0b1f_row5_col1\" class=\"data row5 col1\" >1.000000</td>\n",
       "                        <td id=\"T_c0b1f_row5_col2\" class=\"data row5 col2\" >1</td>\n",
       "                        <td id=\"T_c0b1f_row5_col3\" class=\"data row5 col3\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row6\" class=\"row_heading level0 row6\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B__x10130_y31403_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row6_col0\" class=\"data row6 col0\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B</td>\n",
       "                        <td id=\"T_c0b1f_row6_col1\" class=\"data row6 col1\" >0.010911</td>\n",
       "                        <td id=\"T_c0b1f_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "                        <td id=\"T_c0b1f_row6_col3\" class=\"data row6 col3\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_c0b1f_level0_row7\" class=\"row_heading level0 row7\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B__x10130_y32416_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_c0b1f_row7_col0\" class=\"data row7 col0\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B</td>\n",
       "                        <td id=\"T_c0b1f_row7_col1\" class=\"data row7 col1\" >0.668856</td>\n",
       "                        <td id=\"T_c0b1f_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "                        <td id=\"T_c0b1f_row7_col3\" class=\"data row7 col3\" >MSS</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f64d3e36b50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_ids = [Path(s[0]).name for s in img_dataset.samples]\n",
    "patient_ids = [t.split('__')[0] for t in tile_ids]\n",
    "msi_status = [Path(s[0]).parents[1].name for s in img_dataset.samples]\n",
    "tile_info_df = pd.DataFrame({\n",
    "    'tile_id': tile_ids,\n",
    "    'patient_id': patient_ids,\n",
    "    'tumor_pred_val': all_preds[:, 1].numpy(),\n",
    "    'tumor_pred_class': tumorous_tiles.numpy(),\n",
    "    'MSI_status': msi_status,\n",
    "})\n",
    "tile_info_df.set_index('tile_id', inplace=True)\n",
    "tile_df_save_path = base_save_path / 'tile_info.csv'\n",
    "tile_info_df.to_csv(tile_df_save_path)\n",
    "print(f'Saved tile_info_df to \"{tile_df_save_path}\"')\n",
    "\n",
    "patient_info_df = pd.DataFrame({\n",
    "    'patient_id': tile_info_df['patient_id'].unique(),\n",
    "    'MSI_status': '',\n",
    "})\n",
    "patient_info_df.set_index('patient_id', inplace=True)\n",
    "for patient_id, msi_status in tile_info_df.groupby('patient_id').MSI_status.unique().iteritems():\n",
    "    # Make sure that MSI status is the same for all tiles within a patient\n",
    "    msi_status, = msi_status\n",
    "    patient_info_df.loc[patient_id] = msi_status\n",
    "patient_df_save_path = base_save_path / 'patient_info.csv'\n",
    "patient_info_df.to_csv(patient_df_save_path)\n",
    "print(f'Saved patient_info_df to \"{patient_df_save_path}\"')\n",
    "\n",
    "display(patient_info_df.style.set_caption('Patient info dataframe'))\n",
    "tile_info_df.groupby('patient_id').head(2).style.set_caption('Tile info dataframe example rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Learning Models\n",
    "\n",
    "The deep learning model pipeline consists of three main steps:\n",
    "1. Data splitting\n",
    "1. Model and data loading\n",
    "1. Training loop\n",
    "    1. Performing inference\n",
    "    1. Calculating loss\n",
    "    1. Backpropagating loss\n",
    "    1. Updating parameters\n",
    "    1. Logging results\n",
    "\n",
    "This notebook will also cover a few other important things to consider:\n",
    "1. Common hardware bottlenecks\n",
    "1. Real-time performance monitoring\n",
    "1. Misc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split the patients into a train/validation set and a test set.\n",
    "\n",
    "Normally, 10-20% of patients would be assigned to the test set, but since we only have 4 patients in our example dataset, we will perform a 50/50 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_set, test_set = train_test_split(\n",
    "    patient_info_df.index.values,\n",
    "    test_size=0.5,\n",
    "    stratify=patient_info_df['MSI_status'].values\n",
    ")\n",
    "patient_info_df.loc[train_val_set, 'data_subset'] = 'train/validation'\n",
    "patient_info_df.loc[test_set, 'data_subset'] = 'test'\n",
    "tile_info_df = tile_info_df.drop(\n",
    "    columns='data_subset',\n",
    "    errors='ignore'\n",
    ").join(\n",
    "    patient_info_df['data_subset'],\n",
    "    on='patient_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we split the tiles from the train/validation patients into a train set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved updated tile_info_df to \"tiled_WSIs/tile_info.csv\"\n",
      "Saved updated patient_info_df to \"tiled_WSIs/patient_info.csv\"\n"
     ]
    }
   ],
   "source": [
    "train_val_mask = tile_info_df['data_subset'] != 'test'\n",
    "train_set, val_set = train_test_split(\n",
    "    tile_info_df.index.values[train_val_mask],\n",
    "    train_size=0.9,\n",
    "    stratify=tile_info_df['patient_id'].values[train_val_mask]\n",
    ")\n",
    "tile_info_df.loc[train_set, 'data_subset'] = 'train'\n",
    "tile_info_df.loc[val_set, 'data_subset'] = 'validation'\n",
    "\n",
    "tile_info_df.to_csv(tile_df_save_path)\n",
    "print(f'Saved updated tile_info_df to \"{tile_df_save_path}\"')\n",
    "patient_info_df.to_csv(patient_df_save_path)\n",
    "print(f'Saved updated patient_info_df to \"{patient_df_save_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are no patients with tiles in both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id\n",
       "TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F    [train, validation]\n",
       "TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B                 [test]\n",
       "TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014                 [test]\n",
       "TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7    [train, validation]\n",
       "Name: data_subset, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_info_df.groupby('patient_id')['data_subset'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data and model, and move the model to the correct device.\n",
    "\n",
    "In order to split the train and test sets, and only include tumor images, we define a function to check that a given image path is tumorous and in the correct data subset.\n",
    "\n",
    "The model architectures I've tried and had the most success with are, in no particular order:\n",
    "1. densenet201\n",
    "1. resnet18\n",
    "1. shufflenet_v2_x1_0\n",
    "1. squeezenet1_1\n",
    "\n",
    "However, this is a decision that depends on the amount of data and compute available. For a list of all ImageNet pretrained models available through PyTorch, see https://pytorch.org/vision/stable/models.html.\n",
    "\n",
    "Here we'll use SqueezeNet since it is the smallest and fastest.\n",
    "\n",
    "The choice of optimizer and learning rate is another important hyperparameter choice. For this example we'll use Adam. An excellent overview of various optimizers can be found here: https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "We also define class importance weights based on the inverse of the number of samples in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for training...\n",
      "Loaded 5443 train images.\n",
      "Loaded 632 validation images.\n",
      "Loaded 3731 test images.\n",
      "Loaded model \"squeezenet1_1\" with 723,522 trainable parameters.\n",
      "Class \"MSI-H\" with 4218 samples was given a weight of 0.23.\n",
      "Class \"MSS\" with 1225 samples was given a weight of 0.77.\n"
     ]
    }
   ],
   "source": [
    "# This function defines which files will be allowed in the dataset. Using it, we can subset data without physically moving images into separate folders\n",
    "def get_subset_func(data_subset):\n",
    "    def is_valid_img(path):\n",
    "        if not Path(path).suffix == '.png':\n",
    "            return False\n",
    "        row = tile_info_df.loc[Path(path).name]\n",
    "        return row['tumor_pred_class'] == 1 and row['data_subset'] == data_subset\n",
    "    return is_valid_img\n",
    "\n",
    "# We perform a number of random operations to the images in order to augment the training data\n",
    "# For validation and test data, these transforms are not necessary, but Resize(224) and ToTensor() still are\n",
    "PHASES = ['train', 'validation', 'test']\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomAffine(180, translate=(0.1, 0.1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load the images\n",
    "BATCH_SIZE = 32\n",
    "print('Loading images for training...')\n",
    "img_datasets = {\n",
    "    phase: datasets.ImageFolder(\n",
    "        base_save_path,\n",
    "        transform=data_transforms[phase],\n",
    "        is_valid_file=get_subset_func(phase)\n",
    "    ) for phase in PHASES\n",
    "}\n",
    "img_dataloaders = {\n",
    "    phase: data.DataLoader(\n",
    "        img_datasets[phase],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=8,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    ) for phase in PHASES\n",
    "}\n",
    "num_classes = len(img_datasets['train'].classes)\n",
    "for phase in PHASES:\n",
    "    print(f'Loaded {len(img_datasets[phase])} {phase} images.')\n",
    "\n",
    "# Load the model\n",
    "MODEL_ARCHITECTURE = models.squeezenet1_1\n",
    "model = load_model_arch(MODEL_ARCHITECTURE, pretrained=True, num_classes=2).to(DEVICE)\n",
    "n_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Loaded model \"{MODEL_ARCHITECTURE.__name__}\" with {n_train_params:,d} trainable parameters.')\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "# Get class importance weights\n",
    "class_samples = []\n",
    "class_weights = []\n",
    "for i, c in enumerate(img_datasets['train'].classes):\n",
    "    n_samples = np.sum(np.array(img_datasets['train'].targets) == i)\n",
    "    if n_samples == 0:\n",
    "        raise RuntimeError(f'Class \"{c}\" has no samples. Make sure there are no empty folders in the image dataset folder.')\n",
    "    class_samples.append(n_samples)\n",
    "    class_weights.append(1. / n_samples)\n",
    "class_weights = torch.tensor(class_weights).float()\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "for i, w in enumerate(class_weights.tolist()):\n",
    "    print(f'Class \"{img_datasets[\"train\"].classes[i]}\" with {class_samples[i]} samples was given a weight of {w:.2f}.')\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/100,      train: 100%|██████████| 171/171 [00:05<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100,      train: Loss 0.10\tAcc: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, validation: 100%|██████████| 20/20 [00:00<00:00, 50.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, validation: Loss 0.09\tAcc: 0.99\tAUROC: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100,      train: 100%|██████████| 171/171 [00:05<00:00, 29.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100,      train: Loss 0.06\tAcc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, validation: 100%|██████████| 20/20 [00:00<00:00, 49.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, validation: Loss 0.10\tAcc: 0.96\tAUROC: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100,      train: 100%|██████████| 171/171 [00:05<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100,      train: Loss 0.07\tAcc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, validation: 100%|██████████| 20/20 [00:00<00:00, 51.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, validation: Loss 0.09\tAcc: 0.98\tAUROC: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100,      train: 100%|██████████| 171/171 [00:05<00:00, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100,      train: Loss 0.05\tAcc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, validation: 100%|██████████| 20/20 [00:00<00:00, 50.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, validation: Loss 0.08\tAcc: 0.98\tAUROC: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100,      train: 100%|██████████| 171/171 [00:05<00:00, 30.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100,      train: Loss 0.04\tAcc: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, validation: 100%|██████████| 20/20 [00:00<00:00, 48.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, validation: Loss 0.08\tAcc: 0.99\tAUROC: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100,      train: 100%|██████████| 171/171 [00:05<00:00, 29.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100,      train: Loss 0.05\tAcc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, validation: 100%|██████████| 20/20 [00:00<00:00, 49.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, validation: Loss 0.09\tAcc: 0.99\tAUROC: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100,      train: 100%|██████████| 171/171 [00:05<00:00, 29.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100,      train: Loss 0.05\tAcc: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, validation: 100%|██████████| 20/20 [00:00<00:00, 52.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, validation: Loss 0.09\tAcc: 0.99\tAUROC: 1.00\n",
      "\n",
      "########## Beginning tile-level testing ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100,       test: 100%|██████████| 117/117 [00:01<00:00, 80.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100,       test: Loss 0.72\tAcc: 0.65\tAUROC: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Weights are stored for model checkpointing\n",
    "best_model_weights = copy.deepcopy(model.state_dict())\n",
    "validation_losses = []\n",
    "stop = False\n",
    "dataset_sizes = {p: len(img_datasets[p]) for p in PHASES}\n",
    "\n",
    "# Log various stats for visualization after training\n",
    "log_dict = {}\n",
    "for phase in PHASES:\n",
    "    log_dict[phase + '_acc'] = []\n",
    "    log_dict[phase + '_loss'] = []\n",
    "    if phase != 'train':\n",
    "        log_dict[phase + '_roc_auc'] = []\n",
    "\n",
    "N_EPOCHS = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Cycle through training and validation, only testing the model once all training and validation has finished\n",
    "    for phase in PHASES:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        elif phase == 'validation':\n",
    "            model.eval()\n",
    "            y_true = np.empty((0, num_classes))\n",
    "            y_score = np.empty((0, num_classes))\n",
    "        else:\n",
    "            # Only test after all training and validation has finished\n",
    "            if epoch == N_EPOCHS - 1 or stop:\n",
    "                print('\\n' + '#' * 10 + ' Beginning tile-level testing ' + '#' * 10)\n",
    "                # Reload the best model checkpoint\n",
    "                model.load_state_dict(best_model_weights)\n",
    "                model.eval()\n",
    "                y_true = np.empty((0, num_classes))\n",
    "                y_score = np.empty((0, num_classes))\n",
    "            else:\n",
    "                log_dict[phase + '_acc'].append(None)\n",
    "                log_dict[phase + '_loss'].append(None)\n",
    "                log_dict[phase + '_roc_auc'].append(None)\n",
    "                continue\n",
    "        \n",
    "        running_loss = 0.\n",
    "        running_corrects = 0\n",
    "        for i, (inputs, labels) in tqdm.tqdm(enumerate(img_dataloaders[phase]),\n",
    "                                             desc=f'Epoch {epoch}/{N_EPOCHS}, {phase:>10}',\n",
    "                                             total=len(img_dataloaders[phase])):\n",
    "            # Move the data to the GPU\n",
    "            inputs = inputs.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass, only enabling gradient computation during training\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                loss = loss_func(outputs, labels)\n",
    "                # Backward pass only during training\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    y_true = np.concatenate([y_true, np.eye(num_classes)[labels.cpu().numpy().ravel()]])\n",
    "                    y_score = np.concatenate([y_score, F.softmax(outputs, dim=1).cpu().numpy()])\n",
    "                    \n",
    "            # Track stats for logging\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "            current_loss = running_loss / (i + 1) / BATCH_SIZE\n",
    "        \n",
    "        time.sleep(0.25)\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects.float().cpu().numpy() / dataset_sizes[phase]\n",
    "        epoch_roc_auc = roc_auc_score(y_true, y_score) if phase != 'train' else None\n",
    "        \n",
    "        log_str = f'Epoch {epoch}/{N_EPOCHS}, {phase:>10}: Loss {epoch_loss:.2f}\\tAcc: {epoch_acc:.2f}'\n",
    "        if phase != 'train':\n",
    "            log_str += f'\\tAUROC: {epoch_roc_auc:.2f}'\n",
    "        print(log_str)\n",
    "        \n",
    "        log_dict[phase + '_loss'].append(epoch_loss)\n",
    "        log_dict[phase + '_acc'].append(epoch_acc)\n",
    "        if phase != 'train':\n",
    "            log_dict[phase + '_roc_auc'].append(epoch_roc_auc)\n",
    "        \n",
    "        \n",
    "        if phase == 'validation':\n",
    "            # Save model checkpoint if validation reaches a new minimum\n",
    "            if len(validation_losses) == 0 or epoch_loss < min(validation_losses):\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            validation_losses.append(epoch_loss)\n",
    "            # Stop training if validation performance does not improve for N epochs\n",
    "            N = 3\n",
    "            if np.argmin(validation_losses) < len(validation_losses) - N:\n",
    "                stop = True\n",
    "    if stop:\n",
    "        break\n",
    "log_df = pd.DataFrame(log_dict)\n",
    "log_df.index.rename('Epoch', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print('Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
