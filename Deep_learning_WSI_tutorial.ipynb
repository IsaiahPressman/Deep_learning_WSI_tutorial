{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### TODO: Outline image here###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. [Libraries & Environment](#Libraries-&-Environment)\n",
    "1. [Data Preprocessing](#Data-Preprocessing)\n",
    "    1. Tiling\n",
    "    1. Filtering out background tiles\n",
    "    1. Macenko normalization\n",
    "    1. Tumor detection\n",
    "1. [Training Deep Learning Models](#Training-Deep-Learning-Models)\n",
    "    1. Data splitting\n",
    "    1. Model and data loading\n",
    "    1. Common hardware bottlenecks\n",
    "    1. Real-time performance monitoring\n",
    "    1. Misc.\n",
    "1. [Evaluating Performance](#Evaluating-Performance)\n",
    "    1. Patient-level vs. tile-level evaluation\n",
    "    1. AUROC vs. accuracy\n",
    "    1. On improving performance\n",
    "1. [Visualizing Results](#Visualizing-Results)\n",
    "    1. TODO: outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries & Environment\n",
    "\n",
    "The base environment that I use can be installed using the create_conda_env.sh bash script.\n",
    "\n",
    "NB: As of June 2021, when installing OpenSlide on Linux, it will not work correctly with some image types due to a broken dependency. (I've noticed this problem for .mrxs images in particular) In order to repair this issue, you can install version 0.40.0 of the pixman library. (Installed automatically in the create_conda_env.sh script) If you notice the slide images look like like the image below, or throw an error when you view them, try this solution.\n",
    "\n",
    "TODO: insert image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openslide import OpenSlide, OpenSlideError\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import re\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import time\n",
    "import tqdm\n",
    "import traceback\n",
    "import warnings\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Custom imports\n",
    "from library.MacenkoNormalizer import MacenkoNormalizer\n",
    "from library.model_utils import load_saved_model_for_inference, load_model_arch\n",
    "\n",
    "# DEVICE determines which GPU (or CPU) the deep learning code will be run on\n",
    "# DEVICE = torch.device('cpu')\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In order to prepare the WSI images for deep learning training and inference, a number of preprocessing steps must be applied:\n",
    "\n",
    "1. Images are broken into many small tiles (usually 256x256 microns)\n",
    "1. Tiles are filtered to exclude non-tissue background regions\n",
    "1. Tiles are Macenko-normalized\n",
    "1. Tiles are filtered to exclude non-tumorous tissue regions\n",
    "\n",
    "These steps are laid out in example code below. However, when applying this pipeline at scale, the implementation should include multiprocessing and/or CuPy (for Macenko normalization) as these additions provide enormous speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring WSIs/MSS/TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B.svs, as it has already been processed.\n",
      "Ignoring WSIs/MSS/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs, as it has already been processed.\n",
      "Ignoring WSIs/MSI-H/TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7.svs, as it has already been processed.\n",
      "Ignoring WSIs/MSI-H/TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014.svs, as it has already been processed.\n",
      "Masking and normalizing 0 tiles from 0 whole slide images.\n"
     ]
    }
   ],
   "source": [
    "# MICRONS_PER_TILE defines the tile edge length used when breaking WSIs into smaller images\n",
    "MICRONS_PER_TILE = 256.\n",
    "\n",
    "# Initialize the Macenko Normalizer\n",
    "reference_img = np.array(Image.open('library/macenko_reference_img.png').convert('RGB'))\n",
    "normalizer = MacenkoNormalizer()\n",
    "normalizer.fit(reference_img)\n",
    "\n",
    "# Find all WSIs and check for errors opening the file or finding the microns-per-pixel values \n",
    "base_path = Path('WSIs')\n",
    "base_save_path = Path('tiled_WSIs')\n",
    "wsi_paths = base_path.rglob('*.svs')\n",
    "save_paths = []\n",
    "wsi_paths_to_normalize = []\n",
    "total_num_tiles = 0\n",
    "for wsi_path in wsi_paths:\n",
    "    try:\n",
    "        with OpenSlide(str(wsi_path)) as wsi:\n",
    "            sub_path = Path(str(wsi_path)[len(str(base_path)) + 1:-len(wsi_path.suffix)])\n",
    "            save_path = base_save_path / sub_path\n",
    "\n",
    "            if (save_path / 'Finished.txt').exists():\n",
    "                print('Ignoring {}, as it has already been processed.'.format(wsi_path))\n",
    "            else:\n",
    "                pixels_per_tile_x = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-x']))\n",
    "                pixels_per_tile_y = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-y']))\n",
    "                wsi_paths_to_normalize.append(wsi_path)\n",
    "                save_paths.append(save_path)\n",
    "                save_path.mkdir(parents=True, exist_ok=True)\n",
    "                total_num_tiles += (\n",
    "                        len(range(pixels_per_tile_x, wsi.dimensions[0] - pixels_per_tile_x, pixels_per_tile_x)) *\n",
    "                        len(range(pixels_per_tile_y, wsi.dimensions[1] - pixels_per_tile_y, pixels_per_tile_y)))\n",
    "    except OpenSlideError:\n",
    "        print('Ignoring {}, as it cannot be opened by OpenSlide.'.format(wsi_path))\n",
    "    except KeyError:\n",
    "        print('Ignoring {}, as it does not have a defined microns-per-pixel value'.format(wsi_path))\n",
    "\n",
    "print(f'Masking and normalizing {total_num_tiles} tiles from {len(wsi_paths_to_normalize)} whole slide images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, given a whole slide image path and target save path, masks and normalizes all tissue tiles and then saves them into pngs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_normalize_wsi(wsi_path, save_path, pbar):\n",
    "    num_tiles_kept = 0\n",
    "    try:\n",
    "        with OpenSlide(str(wsi_path)) as wsi:\n",
    "            pptx = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-x']))\n",
    "            ppty = int(MICRONS_PER_TILE / float(wsi.properties['openslide.mpp-y']))\n",
    "            # Leave out border of image\n",
    "            for x in range(pptx, wsi.dimensions[0] - pptx, pptx):\n",
    "                for y in range(ppty, wsi.dimensions[1] - ppty, ppty):\n",
    "                    tile = wsi.read_region((x, y), level=0, size=(pptx, ppty)).convert('RGB')\n",
    "                    # Mask away all-white and all-black background regions\n",
    "                    mask = tile.convert(mode='L').point(lut=lambda p: 220 > p > 10, mode='1')\n",
    "                    mask = ndimage.binary_fill_holes(mask)\n",
    "                    if np.sum(mask).astype(float) / mask.size > 0.5:\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.simplefilter('ignore')\n",
    "                            try:\n",
    "                                # Normalize the tile\n",
    "                                tile = normalizer.transform(np.array(tile))\n",
    "                                tile = Image.fromarray(tile)\n",
    "                                # Resize the image to 224x224\n",
    "                                tile = tile.resize((224, 224), Image.LANCZOS)\n",
    "                                num_tiles_kept += 1\n",
    "                                filename = f'{wsi_path.stem}__x{x}_y{y}_dx{pptx}_dy{ppty}.png'\n",
    "                                tile.save(save_path / filename, format='PNG')\n",
    "                            except np.linalg.LinAlgError:\n",
    "                                pass\n",
    "                    pbar.update()\n",
    "    except OpenSlideError as ex:\n",
    "        print('\\nUnable to process {}:'.format(wsi_path))\n",
    "        print(''.join(traceback.format_exception(etype=type(ex), value=ex, tb=ex.__traceback__)))\n",
    "        shutil.rmtree(save_path)\n",
    "        return 0\n",
    "\n",
    "    with open(save_path / 'Finished.txt', 'w+') as file:\n",
    "        file.write('Kept and processed {} tiles.'.format(num_tiles_kept))\n",
    "    return num_tiles_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2051 tiles from patient TCGA-4N-A93T-01Z-00-DX1 saved to tiled_WSIs/MSS/TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B\n",
      "2960 tiles from patient TCGA-3L-AA1B-01Z-00-DX1 saved to tiled_WSIs/MSS/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F\n",
      "4172 tiles from patient TCGA-5M-AAT6-01Z-00-DX1 saved to tiled_WSIs/MSI-H/TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014\n",
      "6951 tiles from patient TCGA-5M-AATE-01Z-00-DX1 saved to tiled_WSIs/MSI-H/TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7\n",
      "16134 tiles were saved and normalized\n"
     ]
    }
   ],
   "source": [
    "assert len(wsi_paths_to_normalize) == len(save_paths)\n",
    "with tqdm.tqdm(total=total_num_tiles) as pbar:\n",
    "    for wsi_path, save_path in zip(wsi_paths_to_normalize, save_paths):\n",
    "        mask_and_normalize_wsi(wsi_path, save_path, pbar)\n",
    "# Wait a moment for pbar to close\n",
    "time.sleep(0.25)\n",
    "\n",
    "all_save_paths = [p for p in base_save_path.glob('*/*') if p.is_dir()]\n",
    "total_tiles_kept = 0\n",
    "for save_path in all_save_paths:\n",
    "    with open(save_path / 'Finished.txt', 'r') as f:\n",
    "        info = f.readline()\n",
    "    num_tiles_kept = int(re.search('processed ([0-9]+?) tiles', info).group(1))\n",
    "    total_tiles_kept += num_tiles_kept\n",
    "    print(f'{num_tiles_kept} tiles from patient {save_path.stem} saved to {save_path}')\n",
    "print(f'{total_tiles_kept} tiles were saved and normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that WSIs have been broken into normalized tiles, we load these images for tumor detection.\n",
    "\n",
    "NB: Make sure to use `with torch.no_grad():` at inference time or there may be memory overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for tumor detection...\n",
      "Getting tumor predictions for 16134 tiles in 127 batches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:08<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9806/16134 tiles contain tumorous tissue\n"
     ]
    }
   ],
   "source": [
    "print('Loading images for tumor detection...')\n",
    "img_dataset = datasets.ImageFolder(\n",
    "    base_save_path,\n",
    "    transforms.Compose([\n",
    "        # Images must be of size 224x224 to be passed to most deep learning vision models\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "img_dataloader = data.DataLoader(\n",
    "    img_dataset,\n",
    "    batch_size=128,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "tumor_detection_model = load_saved_model_for_inference(\n",
    "    'saved_models/resnet18_tumor_detection_exp9.pt',\n",
    "    num_classes=2,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f'Getting tumor predictions for {len(img_dataset)} tiles in {len(img_dataloader)} batches.')\n",
    "time.sleep(0.25)\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in tqdm.tqdm(img_dataloader):\n",
    "        inputs = inputs.to(DEVICE, non_blocking=True)\n",
    "        outputs = tumor_detection_model(inputs).cpu()\n",
    "        all_preds.append(outputs)\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "time.sleep(0.25)\n",
    "\n",
    "tumorous_tiles = all_preds.argmax(dim=1).flatten()\n",
    "print(f'{tumorous_tiles.sum()}/{len(img_dataset)} tiles contain tumorous tissue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tile_info_df to \"tiled_WSIs/tile_info.csv\"\n",
      "Saved patient_info_df to \"tiled_WSIs/patient_info.csv\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_7b918_\" ><caption>Patient info dataframe</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >MSI_status</th>    </tr>    <tr>        <th class=\"index_name level0\" >patient_id</th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_7b918_level0_row0\" class=\"row_heading level0 row0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014</th>\n",
       "                        <td id=\"T_7b918_row0_col0\" class=\"data row0 col0\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7b918_level0_row1\" class=\"row_heading level0 row1\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7</th>\n",
       "                        <td id=\"T_7b918_row1_col0\" class=\"data row1 col0\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7b918_level0_row2\" class=\"row_heading level0 row2\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F</th>\n",
       "                        <td id=\"T_7b918_row2_col0\" class=\"data row2 col0\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7b918_level0_row3\" class=\"row_heading level0 row3\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B</th>\n",
       "                        <td id=\"T_7b918_row3_col0\" class=\"data row3 col0\" >MSS</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5d04eb4090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_864fa_\" ><caption>Tile info dataframe example rows</caption><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >patient_id</th>        <th class=\"col_heading level0 col1\" >tumor_pred_val</th>        <th class=\"col_heading level0 col2\" >tumor_pred_class</th>        <th class=\"col_heading level0 col3\" >MSI_status</th>    </tr>    <tr>        <th class=\"index_name level0\" >tile_id</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_864fa_level0_row0\" class=\"row_heading level0 row0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014__x100287_y12156_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row0_col0\" class=\"data row0 col0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014</td>\n",
       "                        <td id=\"T_864fa_row0_col1\" class=\"data row0 col1\" >0.010808</td>\n",
       "                        <td id=\"T_864fa_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "                        <td id=\"T_864fa_row0_col3\" class=\"data row0 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row1\" class=\"row_heading level0 row1\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014__x100287_y13169_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row1_col0\" class=\"data row1 col0\" >TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014</td>\n",
       "                        <td id=\"T_864fa_row1_col1\" class=\"data row1 col1\" >0.045893</td>\n",
       "                        <td id=\"T_864fa_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "                        <td id=\"T_864fa_row1_col3\" class=\"data row1 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row2\" class=\"row_heading level0 row2\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7__x100287_y10130_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row2_col0\" class=\"data row2 col0\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7</td>\n",
       "                        <td id=\"T_864fa_row2_col1\" class=\"data row2 col1\" >0.999991</td>\n",
       "                        <td id=\"T_864fa_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "                        <td id=\"T_864fa_row2_col3\" class=\"data row2 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row3\" class=\"row_heading level0 row3\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7__x100287_y11143_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row3_col0\" class=\"data row3 col0\" >TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7</td>\n",
       "                        <td id=\"T_864fa_row3_col1\" class=\"data row3 col1\" >0.999997</td>\n",
       "                        <td id=\"T_864fa_row3_col2\" class=\"data row3 col2\" >1</td>\n",
       "                        <td id=\"T_864fa_row3_col3\" class=\"data row3 col3\" >MSI-H</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row4\" class=\"row_heading level0 row4\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F__x10130_y33429_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row4_col0\" class=\"data row4 col0\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F</td>\n",
       "                        <td id=\"T_864fa_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "                        <td id=\"T_864fa_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "                        <td id=\"T_864fa_row4_col3\" class=\"data row4 col3\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row5\" class=\"row_heading level0 row5\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F__x10130_y34442_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row5_col0\" class=\"data row5 col0\" >TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F</td>\n",
       "                        <td id=\"T_864fa_row5_col1\" class=\"data row5 col1\" >1.000000</td>\n",
       "                        <td id=\"T_864fa_row5_col2\" class=\"data row5 col2\" >1</td>\n",
       "                        <td id=\"T_864fa_row5_col3\" class=\"data row5 col3\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row6\" class=\"row_heading level0 row6\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B__x10130_y31403_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row6_col0\" class=\"data row6 col0\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B</td>\n",
       "                        <td id=\"T_864fa_row6_col1\" class=\"data row6 col1\" >0.010911</td>\n",
       "                        <td id=\"T_864fa_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "                        <td id=\"T_864fa_row6_col3\" class=\"data row6 col3\" >MSS</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_864fa_level0_row7\" class=\"row_heading level0 row7\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B__x10130_y32416_dx1013_dy1013.png</th>\n",
       "                        <td id=\"T_864fa_row7_col0\" class=\"data row7 col0\" >TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B</td>\n",
       "                        <td id=\"T_864fa_row7_col1\" class=\"data row7 col1\" >0.668856</td>\n",
       "                        <td id=\"T_864fa_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "                        <td id=\"T_864fa_row7_col3\" class=\"data row7 col3\" >MSS</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5ce0ca2790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_ids = [Path(s[0]).name for s in img_dataset.samples]\n",
    "patient_ids = [t.split('__')[0] for t in tile_ids]\n",
    "msi_status = [Path(s[0]).parents[1].name for s in img_dataset.samples]\n",
    "tile_info_df = pd.DataFrame({\n",
    "    'tile_id': tile_ids,\n",
    "    'patient_id': patient_ids,\n",
    "    'tumor_pred_val': all_preds[:, 1].numpy(),\n",
    "    'tumor_pred_class': tumorous_tiles.numpy(),\n",
    "    'MSI_status': msi_status,\n",
    "})\n",
    "tile_info_df.set_index('tile_id', inplace=True)\n",
    "tile_df_save_path = base_save_path / 'tile_info.csv'\n",
    "tile_info_df.to_csv(tile_df_save_path)\n",
    "print(f'Saved tile_info_df to \"{tile_df_save_path}\"')\n",
    "\n",
    "patient_info_df = pd.DataFrame({\n",
    "    'patient_id': tile_info_df['patient_id'].unique(),\n",
    "    'MSI_status': '',\n",
    "})\n",
    "patient_info_df.set_index('patient_id', inplace=True)\n",
    "for patient_id, msi_status in tile_info_df.groupby('patient_id').MSI_status.unique().iteritems():\n",
    "    # Make sure that MSI status is the same for all tiles within a patient\n",
    "    msi_status, = msi_status\n",
    "    patient_info_df.loc[patient_id] = msi_status\n",
    "patient_df_save_path = base_save_path / 'patient_info.csv'\n",
    "patient_info_df.to_csv(patient_df_save_path)\n",
    "print(f'Saved patient_info_df to \"{patient_df_save_path}\"')\n",
    "\n",
    "display(patient_info_df.style.set_caption('Patient info dataframe'))\n",
    "tile_info_df.groupby('patient_id').head(2).style.set_caption('Tile info dataframe example rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Learning Models\n",
    "\n",
    "The deep learning model pipeline consists of three main steps:\n",
    "1. Data splitting\n",
    "1. Model and data loading\n",
    "1. Training loop\n",
    "    1. Performing inference\n",
    "    1. Calculating loss\n",
    "    1. Backpropagating loss\n",
    "    1. Updating parameters\n",
    "    1. Logging results\n",
    "\n",
    "This notebook will also cover a few other important things to consider:\n",
    "1. Common hardware bottlenecks\n",
    "1. Real-time performance monitoring\n",
    "1. Misc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split the patients into a train/validation set and a test set.\n",
    "\n",
    "Normally, 10-20% of patients would be assigned to the test set, but since we only have 4 patients in our example dataset, we will perform a 50/50 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_set, test_set = train_test_split(\n",
    "    patient_info_df.index.values,\n",
    "    test_size=0.5,\n",
    "    stratify=patient_info_df['MSI_status'].values\n",
    ")\n",
    "patient_info_df.loc[train_val_set, 'data_subset'] = 'train/validation'\n",
    "patient_info_df.loc[test_set, 'data_subset'] = 'test'\n",
    "tile_info_df = tile_info_df.drop(\n",
    "    columns='data_subset',\n",
    "    errors='ignore'\n",
    ").join(\n",
    "    patient_info_df['data_subset'],\n",
    "    on='patient_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we split the tiles from the train/validation patients into a train set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved updated tile_info_df to \"tiled_WSIs/tile_info.csv\"\n",
      "Saved updated patient_info_df to \"tiled_WSIs/patient_info.csv\"\n"
     ]
    }
   ],
   "source": [
    "train_val_mask = tile_info_df['data_subset'] != 'test'\n",
    "train_set, val_set = train_test_split(\n",
    "    tile_info_df.index.values[train_val_mask],\n",
    "    train_size=0.9,\n",
    "    stratify=tile_info_df['patient_id'].values[train_val_mask]\n",
    ")\n",
    "tile_info_df.loc[train_set, 'data_subset'] = 'train'\n",
    "tile_info_df.loc[val_set, 'data_subset'] = 'validation'\n",
    "\n",
    "tile_info_df.to_csv(tile_df_save_path)\n",
    "print(f'Saved updated tile_info_df to \"{tile_df_save_path}\"')\n",
    "patient_info_df.to_csv(patient_df_save_path)\n",
    "print(f'Saved updated patient_info_df to \"{patient_df_save_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there are no patients with tiles in both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patient_id\n",
       "TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F                 [test]\n",
       "TCGA-4N-A93T-01Z-00-DX1.82E240B1-22C3-46E3-891F-0DCE35C43F8B    [train, validation]\n",
       "TCGA-5M-AAT6-01Z-00-DX1.8834C952-14E3-4491-8156-52FC917BB014    [train, validation]\n",
       "TCGA-5M-AATE-01Z-00-DX1.483FFD2F-61A1-477E-8F94-157383803FC7                 [test]\n",
       "Name: data_subset, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tile_info_df.groupby('patient_id')['data_subset'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data and model, and move the model to the correct device.\n",
    "\n",
    "In order to split the train and test sets, and only include tumor images, we define a function to check that a given image path is tumorous and in the correct data subset.\n",
    "\n",
    "The model architectures I've tried and had the most success with are, in no particular order:\n",
    "1. densenet201\n",
    "1. resnet18\n",
    "1. shufflenet_v2_x1_0\n",
    "1. squeezenet1_1\n",
    "\n",
    "However, this is a decision that depends on the amount of data and compute available. For a list of all ImageNet pretrained models available through PyTorch, see https://pytorch.org/vision/stable/models.html.\n",
    "\n",
    "Here we'll use SqueezeNet since it is the smallest and fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images for training...\n",
      "Loaded 3358 train images.\n",
      "Loaded 373 validation images.\n",
      "Loaded 6075 test images.\n",
      "Loaded model \"squeezenet1_1\" with 723,522 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def get_subset_func(data_subset):\n",
    "    def is_valid_img(path):\n",
    "        if not Path(path).suffix == '.png':\n",
    "            return False\n",
    "        row = tile_info_df.loc[Path(path).name]\n",
    "        return row['tumor_pred_class'] == 1 and row['data_subset'] == data_subset\n",
    "    return is_valid_img\n",
    "\n",
    "PHASES = ['train', 'validation', 'test']\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        # We perform a number of random operations to the images in order to augment the training data\n",
    "        transforms.RandomAffine(180, translate=(0.1, 0.1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'validation': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print('Loading images for training...')\n",
    "img_datasets = {\n",
    "    phase: datasets.ImageFolder(\n",
    "        base_save_path,\n",
    "        transform=data_transforms[phase],\n",
    "        is_valid_file=get_subset_func(phase)\n",
    "    ) for phase in PHASES\n",
    "}\n",
    "img_dataloaders = {\n",
    "    phase: data.DataLoader(\n",
    "        img_datasets[phase],\n",
    "        batch_size=32,\n",
    "        num_workers=8,\n",
    "        shuffle=True,\n",
    "        pin_memory=True\n",
    "    ) for phase in PHASES\n",
    "}\n",
    "for phase in PHASES:\n",
    "    print(f'Loaded {len(img_datasets[phase])} {phase} images.')\n",
    "\n",
    "MODEL_ARCHITECTURE = models.squeezenet1_1\n",
    "model = load_model_arch(MODEL_ARCHITECTURE, pretrained=True, num_classes=2).to(DEVICE)\n",
    "n_train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Loaded model \"{MODEL_ARCHITECTURE.__name__}\" with {n_train_params:,d} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print('Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
